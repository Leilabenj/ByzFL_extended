.. _attacks-label:

Attacks
========

Welcome to the Byzantine Attacks module of the library, which provides implementations of various attack strategies targeting machine learning vectors, typically gradients. This module focuses on the execution of specific attacks designed to manipulate or disrupt the aggregation process of vectors (or gradients) submitted by honest participants.

Explore this module to understand and experiment with a variety of Byzantine attack strategies, enhancing the robustness of your machine learning systems.

.. toctree::
   :caption: Attacks
   :titlesonly:

   classes/sign_flipping
   classes/inner_product_manipulation
   classes/optimal_ipm
   classes/little_is_enough
   classes/optimal_alie
   classes/mimic
   classes/inf
   classes/gaussian